import streamlit as st
import googleapiclient.discovery
import pandas as pd
from datetime import datetime
import os
from io import BytesIO
from PyPDF2 import PdfReader
from dotenv import load_dotenv
import requests
import requests_cache

from youtube_transcript_api import YouTubeTranscriptApi

from openai import OpenAI
from langchain_community.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings  
from langchain_community.vectorstores import FAISS

# Load environment variables
load_dotenv()

# ì•± ì„¤ì •
st.set_page_config(
    page_title="Youtube video analyser",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': None,
        'Report a bug': None, 
        'About': None
    }
)

# Configure API keys
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Initialize APIs
youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
client = OpenAI(api_key=OPENAI_API_KEY)

# ìºì‹œ ì´ˆê¸°í™” (ì˜ˆ: 10ë¶„ TTL)
requests_cache.install_cache('openai_cache', expire_after=600)

# ìœ íŠœë¸Œ ì‡¼ì¸ ì¸ì§€ ì•„ë‹Œì§€ êµ¬ë¶„
def is_youtubeshorts(video_id):
    url = 'https://www.youtube.com/shorts/' + video_id
    req = requests.head(url)
    
    if req.status_code == 200:
        return True
    else:
        return False

# YouTube Transcript APIë¡œ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìš”ì•½
def youtube_transcript(video_id):
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko'])
        first_minute = []
        current_time = 0
        
        for line in transcript:
            if current_time > 180:  # 60ì´ˆ = 1ë¶„
                break
            first_minute.append(line['text'])
            current_time += line['duration']
            
        return ' '.join(first_minute)
    except:
        return ''

# ìœ íŠœë¸Œ ë™ì˜ìƒ ê¸°ë³¸ ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
def fetch_youtube_data(search_query, max_results=50):
    """Fetch YouTube video data for given search query"""
    videos_data = []
    
    # Initial search request
    request = youtube.search().list(
        q=search_query,
        part='snippet',
        type='video',
        maxResults=min(50, max_results)
    )
    response = request.execute()
    
    # Process video IDs to get statistics
    video_ids = [item['id']['videoId'] for item in response['items']]
    stats_request = youtube.videos().list(
        part='statistics',
        id=','.join(video_ids)
    )
    stats_response = stats_request.execute()

    # Get channel IDs for subscriber count
    channel_ids = [item['snippet']['channelId'] for item in response['items']]
    channels_request = youtube.channels().list(
        part='statistics',
        id=','.join(set(channel_ids))  # ì¤‘ë³µ ì œê±°
    )
    channels_response = channels_request.execute()
    
    # ì±„ë„ IDë¥¼ êµ¬ë…ì ìˆ˜ì™€ ë§¤í•‘
    channel_subscribers = {
        channel['id']: int(channel['statistics'].get('subscriberCount', 0))
        for channel in channels_response['items']
    }
    
    # Combine video data with statistics
    for video, stats in zip(response['items'], stats_response['items']):
        views = int(stats['statistics'].get('viewCount', 0))

        if views >= 1000:
            video_id = video['id']['videoId']
            channel_id = video['snippet']['channelId']
            subscriber_count = channel_subscribers.get(channel_id, 0)
            
            # ì¡°íšŒìˆ˜/êµ¬ë…ììˆ˜ ë¹„ìœ¨ ê³„ì‚° (êµ¬ë…ìê°€ 0ì¸ ê²½ìš° ì²˜ë¦¬)
            view_sub_ratio = (views / subscriber_count) * 100 if subscriber_count > 0 else 0

            # ë™ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸
            script = youtube_transcript(video_id)

            # ì‡¼ì¸  ì—¬ë¶€ í™•ì¸
            is_shorts = is_youtubeshorts(video_id)
            
            videos_data.append({
                'title': video['snippet']['title'], 
                'channel': video['snippet']['channelTitle'], 
                'publishedAt': video['snippet']['publishedAt'], 
                'views': views, 
                'subscribers': subscriber_count, 
                'view_sub_ratio': round(view_sub_ratio, 2),  # ì†Œìˆ˜ì  2ìë¦¬ê¹Œì§€ í‘œì‹œ
                'likes': int(stats['statistics'].get('likeCount', 0)), 
                'comments': int(stats['statistics'].get('commentCount', 0)), 
                'description': video['snippet']['description'], 
                'url': f"https://www.youtube.com/watch?v={video['id']['videoId']}", 
                'thumbnail': video['snippet']['thumbnails']['high']['url'], 
                '1min_script': script,  # 1ë¶„ ìš”ì•½ ìŠ¤í¬ë¦½íŠ¸
                'is_shorts': is_shorts  # ì‡¼ì¸ ì¸ì§€ ì•„ë‹Œì§€ êµ¬ë¶„
            })
    
    return pd.DataFrame(videos_data)

# ë¶„ì„
def analyze_with_llm(df, query, context=None):
    """Analyze YouTube data using Claude"""
    # Prepare data summary for Claude
    data_summary = df.to_string()
    
    prompt = f"""ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
ë‹¤ìŒì€ YouTube ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ì •ë³´ì…ë‹ˆë‹¤:
1. ê²€ìƒ‰ì–´: {query}
2. ì´ ì˜ìƒ ìˆ˜: {len(df)}
3. PDF ë¬¸ì„œ í•µì‹¬ ë‚´ìš©: {context if context else ""}  # RAGë¡œ ë¶ˆëŸ¬ì˜¨ contextê°€ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€

- ë°ì´í„°:
{data_summary}

ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. PDF ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ë¶„ì„
- PDFì—ì„œ ì œì‹œëœ ì£¼ìš” ê°œë…/ì£¼ì œê°€ í˜„ì¬ ìœ íŠœë¸Œ ì˜ìƒë“¤ì—ì„œ ì–´ë–»ê²Œ ë‹¤ë¤„ì§€ê³  ìˆëŠ”ì§€
- PDF ë‚´ìš©ê³¼ ë¹„êµí–ˆì„ ë•Œ í˜„ì¬ ìœ íŠœë¸Œ ì˜ìƒë“¤ì˜ ë¶€ì¡±í•œ ì ì´ë‚˜ ì°¨ë³„ì 
- PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì˜ìƒì´ ë‹¤ë¤„ì•¼ í•  í•µì‹¬ ì£¼ì œë‚˜ ê´€ì 

2. ìœ íŠœë¸Œ ë°ì´í„° ë¶„ì„
- ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ì˜ ì „ë°˜ì ì¸ íŠ¸ë Œë“œ
- ê°€ì¥ ì¸ê¸° ìˆëŠ” ì˜ìƒë“¤ì˜ ê³µí†µì 
- ì£¼ìš” ì±„ë„ë“¤ê³¼ ê·¸ë“¤ì˜ ì»¨í…ì¸  íŠ¹ì§•
- ì‹œì²­ì ì°¸ì—¬ë„ê°€ ë†’ì€ ì˜ìƒì˜ íŠ¹ì§•

3. ì œì•ˆì‚¬í•­
ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒì„ ì œì•ˆí•´ì£¼ì„¸ìš”:
- PDF ë‚´ìš©ìœ¼ë¡œ ì œì‘í• í•  ìƒˆë¡œìš´ ë™ì˜ìƒ ì œëª© 3ê°œ ì¶”ì²œ
- ê° ì œëª©ì— ëŒ€í•œ ì²˜ìŒ 2ë¶„ê°„ ì‹¤ì œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- ê¸°ì¡´ ì¸ê¸° ì˜ìƒë“¤ì˜ íŠ¹ì§•ì„ ë°˜ì˜í•œ ì „ë‹¬ ë°©ì‹ ì œì•ˆ

ì°¸ê³ í• ë§Œí•œ í†µê³„:
- í‰ê·  ì¡°íšŒìˆ˜: {df['views'].mean():,.0f}
- í‰ê·  ì¢‹ì•„ìš”: {df['likes'].mean():,.0f}
- í‰ê·  ëŒ“ê¸€ìˆ˜: {df['comments'].mean():,.0f}
- ìµœë‹¤ ì¡°íšŒìˆ˜: {df['views'].max():,}
- ìµœë‹¤ ì¢‹ì•„ìš”: {df['likes'].max():,}"""
    
    try:
        message = client.chat.completions.create(
            model='o1-mini', 
            messages=[
                {'role': 'user', 'content': prompt}
            ], 
            max_completion_tokens=3000, 
        )
        # print('API ì‘ë‹µ:', message)
        
        if message:
            return message.choices[0].message.content
        else:
            print("API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
    
    except Exception as e:
        print(f"Error calling OpenAI API: {str(e)}")
        
        return None


# RAG
def load_pdf(file_stream):
    # PDF ì½ê¸°
    pdf = PdfReader(file_stream)  # íŒŒì¼ì„ ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì™€ì•¼ í•˜ëŠ” ë¬¸ì œë¡œ ì¸í•´ ë‹¤ë¥¸ ë©”ì†Œë“œ ì‚¬ìš©
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = ""
    for page in pdf.pages:
        text += page.extract_text() + "\n"
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    
    # Document ê°ì²´ ìƒì„± ë° ë¶„í• 
    doc = Document(page_content=text)
    documents = text_splitter.split_documents([doc])
    
    return documents

def rag(documents):
    embeddings = HuggingFaceEmbeddings()
    vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)

    return vectorstore

def relevant_context(vectorstore, query, k=3):
    docs = vectorstore.similarity_search(query, k=k)
    context = "\n".join([doc.page_content for doc in docs])

    return context


def main():
    st.title("ğŸ¥ YouTube íŠ¸ë Œë“œ ë¶„ì„ê¸°")
    st.write("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ìœ íŠœë¸Œ ì˜ìƒì„ Claude AIë¡œ ë¶„ì„í•´ë“œë¦½ë‹ˆë‹¤.")
    
    # PDF íŒŒì¼ ì—…ë¡œë“œ
    upload = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
    
    # Search input
    search_query = st.text_input("ë¶„ì„í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    max_results = st.slider("ë¶„ì„í•  ì˜ìƒ ìˆ˜", 10, 50, 30)

    # PDF íŒŒì¼ ì…ë ¥
    vectorstore = None
    if upload:
        with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬ì¤‘ì…ë‹ˆë‹¤..."):
            # ì…ë ¥ëœ íŒŒì¼ ê·¸ëƒ¥
            pdf_stream = BytesIO(upload.getvalue())
            
            chunks = load_pdf(pdf_stream)
            vectorstore = rag(chunks)
            st.success("PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    
    if st.button("ë¶„ì„ ì‹œì‘"):
        with st.spinner("ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # Fetch YouTube data
                df = fetch_youtube_data(search_query, max_results)

                # RAGë¡œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                context = None
                if vectorstore:
                    context = relevant_context(vectorstore, search_query)
                
                # Display basic statistics
                st.subheader("ğŸ“Š ê¸°ë³¸ í†µê³„")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì´ ì¡°íšŒìˆ˜", f"{df['views'].sum():,}")
                with col2:
                    st.metric("í‰ê·  ì¢‹ì•„ìš”", f"{int(df['likes'].mean()):,}")
                with col3:
                    st.metric("í‰ê·  ëŒ“ê¸€", f"{int(df['comments'].mean()):,}")
                
                # Show top videos
                st.subheader("ğŸ”¥ ì¸ê¸° ì˜ìƒ TOP 5")
                top_videos = df.nlargest(5, 'views')

                # ë§Œ ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚´ê¸°
                def format_to_10k(n):
                    num = round(n / 10000, 1)  # ë§Œ ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ê³  ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼

                    return f"{num}ë§Œ"

                top_videos['views'] = top_videos['views'].apply(lambda x: format_to_10k(x) + " ê±´")
                top_videos['subscribers'] = top_videos['subscribers'].apply(lambda x: format_to_10k(x) + " ëª…")
                top_videos['view_sub_ratio'] = top_videos['view_sub_ratio'].apply(lambda x: f"{round(x)}%")  # ì •ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
                
                # ì¸ë„¤ì¼ì— ë§í¬ ì¶”ê°€
                top_videos['thumbnail'] = top_videos.apply(lambda x: f'<a href="{x["url"]}" target="_blank"><img src="{x["thumbnail"]}" width="240"/></a>', axis=1)
                # top_videos['thumbnail'] = top_videos['thumbnail'].apply(lambda x: x)

                # ë³´ì—¬ì¤„ ì»¬ëŸ¼ ì„ íƒ ë° ì´ë¦„ ë³€ê²½ (ë§í¬ ì—´ ì œì™¸)
                display_videos = top_videos[['thumbnail', 'title', 'channel', 'views', 'subscribers', 'view_sub_ratio', 'is_shorts', '1min_script']]
                display_videos.columns = ['ì¸ë„¤ì¼', 'ì œëª©', 'ì±„ë„ëª…', 'ì¡°íšŒìˆ˜', 'êµ¬ë…ììˆ˜', 'ì¡°íšŒìˆ˜/êµ¬ë…ì ë¹„ìœ¨', 'ì‡¼ì¸ ', 'ìµœì´ˆ 1ë¶„ ìŠ¤í¬ë¦½íŠ¸']

                display_videos['ì‡¼ì¸ '] = display_videos['ì‡¼ì¸ '].map({True: 'ì‡¼ì¸ ', False: 'ë¡±í¼'})

                # HTMLì„ í—ˆìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                st.markdown(display_videos.to_html(escape=False, index=False), unsafe_allow_html=True)
                
                # Engagement rate calculation
                df['engagement_rate'] = (df['likes'] + df['comments']) / df['views'] * 100
                
                # Additional statistics
                st.subheader("ğŸ“ˆ ì°¸ì—¬ë„ ë¶„ì„")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  ì°¸ì—¬ìœ¨", f"{df['engagement_rate'].mean():.2f}%")
                with col2:
                    st.metric("ìµœê³  ì°¸ì—¬ìœ¨", f"{df['engagement_rate'].max():.2f}%")
                
                # Claude Analysis
                st.subheader("ğŸ¤– Claude AI ë¶„ì„ ë¦¬í¬íŠ¸")
                analysis = analyze_with_llm(df, search_query, context)  # ë¶„ì„!
                st.markdown(analysis)
                print('analysis:\n', analysis)
                
                # Data visualization
                st.subheader("ğŸ“Š ë°ì´í„° ì‹œê°í™”")
                tab1, tab2 = st.tabs(["ì¡°íšŒìˆ˜ ë¶„í¬", "ì°¸ì—¬ë„ ë¶„í¬"])
                
                with tab1:
                    st.bar_chart(df.nlargest(10, 'views')[['title', 'views']].set_index('title'))
                
                with tab2:
                    st.bar_chart(df.nlargest(10, 'engagement_rate')[['title', 'engagement_rate']].set_index('title'))
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    main()
